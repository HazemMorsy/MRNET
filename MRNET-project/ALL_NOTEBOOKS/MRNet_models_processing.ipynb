{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MRNet_models_processing.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sjwQJbS4DNac","colab_type":"text"},"source":["## **All needed library imports**"]},{"cell_type":"code","metadata":{"id":"EQvR4fkNDRyq","colab_type":"code","outputId":"6d0a5bb7-8549-4879-9494-f57f74db7570","executionInfo":{"status":"ok","timestamp":1590446397894,"user_tz":-120,"elapsed":6488,"user":{"displayName":"sherif mohamed","photoUrl":"","userId":"07338278518613201610"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.models import Model\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","import pandas as pd\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"aqvYMolL1w_D","colab_type":"text"},"source":["# **All needed labels and constants**"]},{"cell_type":"code","metadata":{"id":"SnCBigQXyCjd","colab_type":"code","colab":{}},"source":["# Here we define the labels to be used in obtaining the data\n","# series\n","axial = 'axial';\n","coronal = 'coronal';\n","sagittal = 'sagittal';\n","# data set\n","valid = 'valid';\n","train = 'train';\n","# symptom\n","acl = 'acl';\n","meniscal = 'meniscus';\n","abnormal = 'abnormal';\n","# models\n","vgg = 'VGG16';\n","inception = 'Inception V3';\n","resnet = 'Resnet';\n","densenet = 'DenseNet';\n","contrust = 'contrust'\n","trans_resnet = 'trans_ResNet';\n","vgg_transfer = 'VGG16_transfer_learning';\n","inception_transfer = 'InceptionV3_transfer_learning'\n","# model typesS\n","extractor = 'Extractor';\n","classifier = 'Classifier';\n","regressor = 'Regressor';\n","# paths\n","path_data = '/content/drive/My Drive/MRNET data set/MRNet-v1.0';\n","path_model = '/content/drive/My Drive/Models';\n","delim = '/';\n","# extensions\n","extension_model = '.h5';\n","extension_numpy = '.npy';\n","extension_csv = '.csv';\n","# total data\n","total_train = 1130;\n","total_test = 120;"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0ITqvN9Pxq1E","colab_type":"text"},"source":["# **Obtaining the data for training in a correct format**"]},{"cell_type":"code","metadata":{"id":"DDisfeedicdn","colab_type":"code","outputId":"c43fa386-83ea-490f-c330-78f000e87102","executionInfo":{"status":"ok","timestamp":1590344208639,"user_tz":-120,"elapsed":1050,"user":{"displayName":"sherif mohamed","photoUrl":"","userId":"07338278518613201610"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# importing the drive to obtain the data files\n","from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lW7xpE0nFUH9","colab_type":"code","colab":{}},"source":["# unzipping the data zip file run once\n","!unzip '/content/drive/My Drive/MRNET data set/MRNet-v1.0.zip' -d '/content/drive/My Drive/MRNET data set'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L_sk_qq714vT","colab_type":"code","colab":{}},"source":["# This function is used to obtain the data from drive to memory for training\n","def get_data_train(series,anomaly,start = 0,batch_size = total_train):\n","  input_data = [];\n","  for exam_no in range(start,batch_size + start):\n","      path_input = path_data + delim  + train + delim + series + delim + format(exam_no,'04d') + extension_numpy;\n","      input_data.append(np.stack([np.load(path_input)] * 3, axis = 3));\n","  path_output = path_data + delim + train + '-' + anomaly + extension_csv;\n","  output_data = np.genfromtxt(path_output, delimiter= ',')[:,1].astype(int);\n","  return input_data, output_data[start:batch_size + start];"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8e5s8drLFYwL","colab_type":"code","colab":{}},"source":["# This function is used to obtain the data from drive to memory for testing\n","def get_data_test(series,anomaly,start = 0,batch_size = total_test):\n","  input_data = [];\n","  for iterator in range(start,batch_size + start):\n","      exam_no = iterator + total_train;\n","      path_input = path_data + delim  + valid + delim + series + delim + format(exam_no,'04d') + extension_numpy;\n","      input_data.append(np.stack([np.load(path_input)] * 3, axis = 3));\n","  path_output = path_data + delim + valid + '-' + anomaly + extension_csv;\n","  output_data = np.genfromtxt(path_output, delimiter= ',')[:,1].astype(int);\n","  return input_data, output_data[start:batch_size + start];"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ByiaFQDs13YQ","colab_type":"text"},"source":["# **Data normalization part**"]},{"cell_type":"code","metadata":{"id":"dJA7UyZs18MJ","colab_type":"code","colab":{}},"source":["# normalizing the image pixels to be a number from 0 to 1\n","def normalize(X):\n","  i = 0;\n","  for curr in X:\n","    X[i] = curr/255;\n","  return X;"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"17OHcLkqYLQY","colab_type":"text"},"source":["# **Data augmentation**"]},{"cell_type":"code","metadata":{"id":"wQU_i1J8YQcv","colab_type":"code","colab":{}},"source":["# based on augmentation techniques defined in the paper, used in training the extractor as the data set is smaller\n","def get_generators():\n","  train_datagen = ImageDataGenerator(\n","      rotation_range=25,\n","      width_shift_range=[-25,25],\n","      height_shift_range=[-25,25],\n","      horizontal_flip=True);\n","  validation_datagen = ImageDataGenerator();\n","  return train_datagen,validation_datagen;"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PsEVz5V8CB-K","colab_type":"text"},"source":["# **Data splitting part**"]},{"cell_type":"code","metadata":{"id":"L6mSwzflCHLi","colab_type":"code","colab":{}},"source":["# splitting the data to train and validate with ratios 90% to 10%\n","def split(X , Y):\n","  return X[:1017],Y[:1017],X[1017:],Y[1017:];"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SK4uMa5xI90O","colab_type":"text"},"source":["# **Training functions:**"]},{"cell_type":"code","metadata":{"id":"67C5BfqJJCgD","colab_type":"code","colab":{}},"source":["def train_extractor(model:Model,model_type,series,anomaly):\n","  # obtaining the data\n","  input_train,output_train = get_data_train(series,anomaly);\n","  input_train,output_train,input_validate,output_validate = split(input_train,output_train);\n","  # processing to train for the extractor specifically: obtaining only one image from each patient\n","  input_train = get_one_image(input_train);\n","  input_validate = get_one_image(input_validate);\n","  # performing data augmentation\n","  train_datagen,validation_datagen = get_generators();\n","  train_generator = train_datagen.flow(input_train,output_train, batch_size = 20);\n","  validation_generator = validation_datagen.flow(input_validate,output_validate, batch_size = 20);\n","  # defining the call backs for training\n","  save_path = path_model + delim + model_type + delim + extractor + delim+series + '_' + anomaly + extension_model;\n","  save_best = ModelCheckpoint(save_path, monitor='val_acc', mode='max', verbose=2, save_best_only=True);\n","  stop = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=5);\n","  # performing a traininig operation with a batch size to overcome any overfitting\n","  training_history = model.fit_generator(train_generator, validation_data=validation_generator, epochs=50, callbacks=[save_best,stop]);\n","  # plotting the graph for the training history\n","  plot(training_history);\n","  return training_history;"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uwhjnY5nN2hH","colab_type":"code","colab":{}},"source":["# This function is used to get the first image from the S slices of the given images\n","# this is used in training the extractor according to our approach\n","def get_one_image(input_train):\n","  new_input_train = [];\n","  for curr in input_train:\n","     new_input_train.append(curr[0]);\n","  return np.array(new_input_train);"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gvHgIA-tjsSa","colab_type":"code","colab":{}},"source":["def train_classifier(extractor:Model,binary_classifier:Model,model_type,series,anomaly):\n","  # obtaining the data\n","  input_train,output_train = get_data_train(series,anomaly);\n","  input_train,output_train,input_validate,output_validate = split(input_train,output_train);\n","  # obtaining the data specifically for the classifier by getting the extracted features\n","  input_train = get_features(extractor,input_train);\n","  input_validate = get_features(extractor,input_validate);\n","  # defining the call backs for training\n","  save_path = path_model + delim + model_type + delim + classifier + delim + series + '_' + anomaly + extension_model;\n","  save_best = ModelCheckpoint(save_path, monitor='val_acc', mode='max', verbose=2, save_best_only=True);\n","  stop = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=5);\n","  # performing a traininig operation with a batch size to overcome any overfitting\n","  training_history = binary_classifier.fit(x=input_train,y = output_train, validation_data=(input_validate,output_validate), epochs=50,batch_size = 20, callbacks=[save_best,stop]);\n","  # plotting the graph for the training history\n","  plot(training_history);\n","  return training_history;"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"co61bPJokios","colab_type":"code","colab":{}},"source":["def get_features(extractor:Model,input):\n","  # forming the extraction model\n","  model = keras.models.Sequential();\n","  model.add(extractor);\n","  # adding the gloabal average layer as in the MRnet specification\n","  model.add(keras.layers.GlobalAveragePooling2D(data_format='channels_last'));\n","  # extracting the features\n","  result = [];\n","  for curr in input:\n","    # getting the predictions\n","    features_extracted = model.predict(curr);\n","    # getting maximum between the batches\n","    max_features = np.max(features_extracted,axis=0);  \n","    result.append(max_features);\n","  return np.array(result);"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NtlVeEI450Xu","colab_type":"code","colab":{}},"source":["def train_regressor(logistic_regressor:Model,extractor_axial:Model,extractor_sagittal:Model,extractor_coronal:Model,model_type,anomaly):\n","  # predictions of all classifiers:\n","  pred_axial,output_train,val_axial,output_validate = get_classifications(extractor_axial,model_type,axial,anomaly);\n","  pred_sagittal,output_train,val_sagittal,output_validate = get_classifications(extractor_sagittal,model_type,sagittal,anomaly);\n","  pred_coronal,output_train,val_coronal,output_validate = get_classifications(extractor_coronal,model_type,coronal,anomaly);\n","  # combining the predictions\n","  input_train = np.array([pred_axial,pred_coronal,pred_sagittal]).reshape(3,pred_axial.shape[0]).T;\n","  input_validate = np.array([val_axial,val_coronal,val_sagittal]).reshape(3,val_axial.shape[0]).T;\n","  # defining the call backs for training\n","  save_path = path_model + delim + model_type + delim + regressor + delim + anomaly + extension_model;\n","  save_best = ModelCheckpoint(save_path, monitor='val_acc', mode='max', verbose=2, save_best_only=True);\n","  stop = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=5);\n","  # performing a traininig operation with a batch size to overcome any overfitting\n","  training_history = logistic_regressor.fit(x=input_train,y = output_train, validation_data=(input_validate,output_validate), epochs=50,batch_size = 20, callbacks=[save_best,stop]);\n","  # plotting the graph for the training history\n","  plot(training_history);\n","  return training_history;\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mvjxrjGg6FMk","colab_type":"code","colab":{}},"source":["def get_classifications(extractor:Model,model_type,series,anomaly):\n","  # obtaining the data\n","  input_train,output_train = get_data_train(series,anomaly);\n","  input_train,output_train,input_validate,output_validate = split(input_train,output_train);\n","  # obtaining the features\n","  input_train = get_features(extractor,input_train);\n","  input_validate = get_features(extractor,input_validate);\n","  # load the classifier\n","  binary_classifier = load_model(model_type,series,anomaly,classifier);\n","  # obtaining the cassifications\n","  input_train = binary_classifier.predict(input_train);\n","  input_validate = binary_classifier.predict(input_validate);\n","  return input_train,output_train,input_validate,output_validate;"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fEHXjJJcPfUX","colab_type":"text"},"source":["# **Functions for model evaluation:**"]},{"cell_type":"code","metadata":{"id":"8_Fx0T_TPjMB","colab_type":"code","colab":{}},"source":["def plot(history):\n","  # drawing the model graph\n","  pd.DataFrame(history.history).plot(figsize=(10, 7));\n","  plt.grid(True);\n","  plt.gca().set_ylim(0, 1);\n","  plt.show();"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aHWzlCbpQZja","colab_type":"code","colab":{}},"source":["def test_extractor(model:Model,series,anomaly):\n","  # getting test data\n","  input_test,output_test = get_data_test(series,anomaly);\n","  # processing it to test the extractor specifically: getting one image from each patient only\n","  input_test = get_one_image(input_test);\n","  # normalization\n","  input_test = normalize(input_test);  \n","  return model.evaluate(input_test,output_test);"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bPBNP7JNmNd_","colab_type":"code","colab":{}},"source":["def test_classifier(extractor:Model,classifier:Model,series,anomaly):\n","  # getting test data\n","  input_test,output_test = get_data_test(series,anomaly);\n","  # processing it to test the extractor specifically: getting one image from each patient only\n","  input_test = get_features(extractor,input_test);\n","  return classifier.evaluate(input_test,output_test);"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SASl0M4V-PXe","colab_type":"code","colab":{}},"source":["def test_regressor(extractor_axial:Model,extractor_sagittal:Model,extractor_coronal:Model,model_type,anomaly):\n","  # getting the regressor\n","  logistic_regressor = load_regressor(model_type,anomaly,regressor);\n","  # getting the test classifications\n","  pred_axial,output_test = get_classifications_test(extractor_axial,model_type,axial,anomaly);\n","  pred_sagittal,output_test = get_classifications_test(extractor_sagittal,model_type,sagittal,anomaly);\n","  pred_coronal,output_test = get_classifications_test(extractor_coronal,model_type,coronal,anomaly);\n","  # combining the predictions\n","  input_test = np.array([pred_axial,pred_coronal,pred_sagittal]).reshape(3,pred_axial.shape[0]).T;\n","  return logistic_regressor.evaluate(input_test,output_test);\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ytG2jxdc_PMw","colab_type":"code","colab":{}},"source":["def get_classifications_test(extractor:Model,model_type,series,anomaly):\n","  # obtaining the data\n","  input_test,output_test = get_data_test(series,anomaly);\n","  # obtaining the features\n","  input_test = get_features(extractor,input_test);\n","  # load the classifier\n","  binary_classifier = load_model(model_type,series,anomaly,classifier);\n","  # obtaining the cassifications\n","  input_test = binary_classifier.predict(input_test);\n","  return input_test,output_test;"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"elQZ9eIM6Yok","colab_type":"text"},"source":["# **Functions to get the model predictions**"]},{"cell_type":"code","metadata":{"id":"TIeW8-i86es_","colab_type":"code","colab":{}},"source":["# extractors is a List of the saved extractors containing: axial-abnormal,sagittal-abnormal,coronal-abnormal,axial-acl,\n","# sagittal-acl coronal-acl,axial-meniscal,sagittal-meniscal,coronal-meniscal.\n","def predict(extractors:list,input_axial,input_sagittal,input_coronal,model_type):\n","  inputs = [input_axial,input_sagittal,input_coronal];\n","  # getting the prediction for each anomaly\n","  pred_abnormal = predict_anomaly(extractors[0:3],inputs,abnormal,model_type);\n","  pred_acl = predict_anomaly(extractors[3:6],inputs,acl,model_type);\n","  pred_meniscal = predict_anomaly(extractors[6:9],inputs,meniscal,model_type);\n","  # combining the predictions and multiplying by 100\n","  predictions = [pred_abnormal,pred_acl,pred_meniscal] * 100;\n","  # printing the predictions\n","  str_abnormal = abnormal + ' : ' +  str(predictions[0]) + '%';\n","  str_acl = acl + ' : ' + str(predictions[1]) + '%';\n","  str_meniscal = meniscal +' : ' + str(predictions[2]) + '%';\n","  print(str_abnormal);\n","  print(str_acl);\n","  print(str_meniscal);"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z6CzIWZm7iLO","colab_type":"code","colab":{}},"source":["def predict_anomaly(extractors:list,inputs:list,anomaly,model_type):\n","  # getting the classifiers\n","  classifier_axial = load_model(model_type,axial,anomaly,classifier);\n","  classifier_sagittal = load_model(model_type,sagittal,anomaly,classifier);\n","  classifier_coronal = load_model(model_type,coronal,anomaly,classifier);\n","  # getting the regressor\n","  logistic_regressor = load_regressor(model_type,anomaly,regressor);\n","  # getting classifications\n","  pred_axial = get_classification_single(extractors[0],classifier_axial,inputs[0]);\n","  pred_sagittal = get_classification_single(extractors[1],classifier_sagittal,inputs[1]);\n","  pred_coronal = get_classification_single(extractors[2],classifier_coronal,inputs[2]);\n","  # combining the predictions\n","  input_regressor = np.array([pred_axial,pred_coronal,pred_sagittal]).reshape(3,pred_axial.shape[0]).T;\n","  # final prediction is returned\n","  return logistic_regressor.predict(input_regressor);"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lnyEpeQB-UoB","colab_type":"code","colab":{}},"source":["def get_classification_single(extractor:Model,binary_classifier:Model,input_single):\n","  # obtaining the features\n","  result = get_features(extractor,input_single);\n","  # obtaining the cassifications\n","  result = binary_classifier.predict(result);\n","  return result;"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vuHJEl2sR6oF","colab_type":"text"},"source":["# **Functions to manipualte saved models**"]},{"cell_type":"code","metadata":{"id":"tLK3NnZ1R-eF","colab_type":"code","colab":{}},"source":["# loads a model of type classifier or extractor\n","def load_model(model_type,series,anomaly,model_part):\n","  model_path = path_model + delim + model_type + delim + model_part + delim + series + '_' + anomaly + extension_model;\n","  return tf.keras.models.load_model(model_path);"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xP43Ly7-8Dyh","colab_type":"code","colab":{}},"source":["# loads a regressor\n","def load_regressor(model_type,anomaly,model_part):\n","  model_path = path_model + delim + model_type + delim + model_part + delim + anomaly + extension_model;\n","  return tf.keras.models.load_model(model_path);"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WEabHC4HztjN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}